{"cells": [{"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "editable": false, "jupyter": {"outputs_hidden": true, "source_hidden": true}}, "outputs": [], "source": ["# Please do not change this cell because some hidden tests might depend on it.\n", "import os\n", "\n", "# Otter grader does not handle ! commands well, so we define and use our\n", "# own function to execute shell commands.\n", "def shell(commands, warn=True):\n", "    \"\"\"Executes the string `commands` as a sequence of shell commands.\n", "     \n", "       Prints the result to stdout and returns the exit status. \n", "       Provides a printed warning on non-zero exit status unless `warn` \n", "       flag is unset.\n", "    \"\"\"\n", "    file = os.popen(commands)\n", "    print (file.read().rstrip('\\n'))\n", "    exit_status = file.close()\n", "    if warn and exit_status != None:\n", "        print(f\"Completed with errors. Exit status: {exit_status}\\n\")\n", "    return exit_status\n", "\n", "shell(\"\"\"\n", "ls requirements.txt >/dev/null 2>&1\n", "if [ ! $? = 0 ]; then\n", " rm -rf .tmp\n", " git clone https://github.com/cs187-2021/lab1-2.git .tmp\n", " mv .tmp/tests ./\n", " mv .tmp/requirements.txt ./\n", " rm -rf .tmp\n", "fi\n", "pip install -q -r requirements.txt\n", "\"\"\")"]}, {"cell_type": "code", "execution_count": null, "id": "6f3a9fd8", "metadata": {"deletable": false, "editable": false}, "outputs": [], "source": ["# Initialize Otter\n", "import otter\n", "grader = otter.Notebook()"]}, {"cell_type": "markdown", "metadata": {"tags": ["remove_for_latex"]}, "source": ["# CS187\n", "## Lab 1-2 \u2014 Text classification and evaluation methodology"]}, {"cell_type": "markdown", "metadata": {}, "source": ["After this lab, you should be able to\n", "\n", "* Understand the distinction between training and test corpora, and why both are needed;\n", "* Understand the role of gold labels;\n", "* Implement a majority class baseline as a benchmark to compare other methods;\n", "* Implement nearest neighbor classification, and understand the role of distance metrics in its operation;\n", "* Compare multiple methods for acccuracy."]}, {"cell_type": "markdown", "metadata": {}, "source": ["New bits of Python used for the first time in the _solution set_ for this lab, and which you may therefore find useful, include\n", "\n", "* [`collections.Counter`](https://docs.python.org/3/library/collections.html#collections.Counter)\n", "* [`collections.Counter.most_common`](https://docs.python.org/3/library/collections.html#collections.Counter.most_common)\n", "* [`torch.float`](https://pytorch.org/docs/stable/tensors.html)\n", "* [`torch.Tensor.type`](https://pytorch.org/docs/stable/generated/torch.Tensor.type.html?highlight=torch%20tensor%20type#torch.Tensor.type)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Preparation \u2013 Loading packages and data"]}, {"cell_type": "code", "execution_count": null, "metadata": {"deletable": false, "editable": false, "jupyter": {"source_hidden": true}}, "outputs": [], "source": ["# Please do not change these imports because some hidden tests might depend on them.\n", "# You can add a cell below if you need to import anything else.\n", "import collections\n", "import copy\n", "import json\n", "import math\n", "from pprint import pprint\n", "import torch"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# The Federalist Papers\n", "\n", "<img src=\"https://github.com/nlp-course/data/raw/master/Federalist/federalist.jpg\" width=150 align=right />\n", "\n", "The _Federalist_ papers is a collection of 85 essays written pseudonymously by Alexander Hamilton, John Jay, and James Madison following the Constitutional Convention of 1787, promoting the ratification of the nascent United States Constitution.\n", "\n", "The authorship of many of the individual papers has been well established and acknowledged by the various authors, but a number of the papers have been contentious, with both Madison and Hamilton as possible authors. Determining the authorship of these disputed papers is a classic text classification problem, and one that has received great attention. The seminal work on the problem is that of [Mosteller and Wallace](http://www.historyofinformation.com/detail.php?entryid=4799), who applied then-novel statistical methods to the problem. In this lab, we'll use the _Federalist_ data to experiment with some of the ideas about distance metrics and classification methods that you've read about. (It's also an excuse to make some points about proper testing methodology.)\n", "\n", "Mosteller and Wallace used the frequencies of various words in the papers as the raw data for determining authorship. We've provided access to a heavily pre-digested version of this data. (If you're interested, you can find the raw data \u2013 all 85 papers \u2013 and the notebook used to generate the pre-digested data in the [course `data` github repository](https://github.com/nlp-course/data).)\n", "\n", "Start by evaluating the cells below to load the data and view a sample."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Retrieve the Federalist data\n", "shell('wget -nv -N -P data https://github.com/nlp-course/data/raw/master/Federalist/federalist_data.json')\n", "# Read the json data into a data structure\n", "with open('data/federalist_data.json', 'r') as fin:\n", "    dataset = json.load(fin)\n", "# Convert counts to tensors of floats\n", "for example in dataset:\n", "    example['counts'] = torch.tensor(example['counts']).type(torch.float)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# View a sample of the data\n", "print(f\"Number of papers in the dataset: {len(dataset)}\")\n", "print(\"Some examples:\")\n", "pprint(dataset[:3])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["You'll see above that the dataset is a list of *examples*, one for each paper, each a dictionary providing the paper number, its title and author(s), and the raw counts for a few important words in the papers. From the last lab, you'll recognize the `counts` field as a bag-of-words representation of the document. The `counts` field is the document representation that we will be wanting to classify, and the `authors` field contains the pertinent class label for each example. \n", "\n", "For your reference, here are the words that were used to derive the counts:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["keywords = ['on', 'upon', 'there', 'whilst']"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Thus in the first example paper, *Federalist 1*, there were 9 tokens of \"on\", 6 of \"upon\", 2 of \"there\", and none of \"whilst\". \n", "\n", "The `authors` field takes on various values. Here's a table of the frequency of each of the values. (This will come in handy later.)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Generate a table of the number of papers by each author label\n", "cnt = collections.Counter(map(lambda ex: ex['authors'],\n", "                              dataset))\n", "for author, count in cnt.items():\n", "    print(f\"{count:3d} ({100.0*count/len(dataset):.3f}%) {author}\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["As you can see, some of the papers are of known authorship by one of Madison or Hamilton. We can use these as training data."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Extract the papers by either of Madison and Hamilton\n", "training = list(filter(lambda ex: ex['authors'] in ['Madison', 'Hamilton'],\n", "                       dataset))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# View a sample of the training data\n", "print(f\"Number of papers in the dataset: {len(training)}\")\n", "print(\"Some examples:\")\n", "pprint(training[:3])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Others of the papers are of ambiguous authorship. They are shown as having `'Hamilton or Madison'` as author. These will be the elements that we want to test our models on."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# Extract the papers of unknown authorship\n", "testing = list(filter(lambda ex: ex['authors'] == 'Hamilton or Madison',\n", "                      dataset))"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["# View a sample of the data\n", "print(f\"Number of papers in the dataset: {len(testing)}\")\n", "print(\"Some sample elements:\")\n", "pprint(testing[:3])"]}, {"cell_type": "markdown", "metadata": {}, "source": ["# Models for text classification"]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["We can think of a _model_ for a text classification problem as a function taking a test example and returning a class label for the test example. Generating the model will rely on a corpus of training data.\n", "\n", "With a model in hand, we can evaluate its _accuracy_ on a test corpus by computing the proportion of test examples that the model correctly classifies, that is, the model assigns to a test example the author that the test example specifies. Define a higher-order function `accuracy` that takes a test corpus (like `testing`) and a model (which is a function, remember), and returns the accuracy of the model on that corpus. \n", "\n", "> For you CS51 aficionados, `accuracy` is a _higher-order function_ since it _takes a function as an argument_. Yes, [higher-order functions are possible in Python](https://en.wikipedia.org/wiki/Higher-order_function#Python).\n", "<!--\n", "BEGIN QUESTION\n", "name: accuracy\n", "-->"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#TODO -- Define the `accuracy` function.\n", "def accuracy(test_corpus, model):\n", "    \"\"\"Computes the accuracy of a model on a corpus.\n", "    Arguments:\n", "      `test_corpus`: a list of test examples, such as `testing`\n", "      `model`: a function whose input is an example from the corpus (such as \n", "              `testing[0]`, and whose output is the predicted author\n", "    Returns:\n", "      accuracy, a float number.\n", "    \"\"\"\n", "    ..."]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["## Majority class classification\n", "\n", "An especially simple classification model labels each test example with whichever label happens to occur most frequently in the training data. It completely ignores the test example that it classifies!\n", "\n", "By examination of the table provided above, what is the majority class label for the training dataset?\n", "\n", "<!--\n", "BEGIN QUESTION\n", "name: maj_class_label\n", "-->"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#TODO -- Set this variable to the majority class label for the training set.\n", "maj_class_label = ..."]}, {"cell_type": "code", "execution_count": null, "id": "977b0c6a", "metadata": {"deletable": false, "editable": false}, "outputs": [], "source": ["grader.check(\"maj_class_label\")"]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["Rather than determining the majority class by inspection, it's better to have a function to compute it for us. Define a function `majority_class_label` that returns the majority class label for a training set.\n", "<!--\n", "BEGIN QUESTION\n", "name: majority_class_label\n", "-->"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#TODO -- Define the `majority_class_label` function.\n", "def majority_class_label(training):\n", "    \"\"\"Find the majority class label for a training set.\n", "    Arguments:\n", "      `training`: a list of training examples, such as `training`\n", "    Returns:\n", "      the majority class label, a string.\n", "    \"\"\"\n", "    ..."]}, {"cell_type": "code", "execution_count": null, "id": "115d5745", "metadata": {"deletable": false, "editable": false}, "outputs": [], "source": ["grader.check(\"majority_class_label\")"]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["What proportions of the *training* examples do you think would be classified correctly by the majority class model?\n", "<!--\n", "BEGIN QUESTION\n", "name: maj_class_accuracy_guess\n", "-->"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#TODO -- Define this variable to be what you think the \n", "#        accuracy of the majority class model would be\n", "#        on the training data.\n", "maj_class_accuracy_guess = ..."]}, {"cell_type": "code", "execution_count": null, "id": "84eee08d", "metadata": {"deletable": false, "editable": false}, "outputs": [], "source": ["grader.check(\"maj_class_accuracy_guess\")"]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["Now define a function `majority_class` that takes a single argument (a test example) and returns the particular class label that is most frequent in the training data `training` (regardless of what the test example is).\n", "<!--\n", "BEGIN QUESTION\n", "name: majority_class\n", "-->"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#TODO - Define the `majority_class` model.\n", "def majority_class(example):\n", "    \"\"\"Defines a majority class model.\n", "    Arguments:\n", "      `example`: an example, such as `testing[0]`\n", "    Returns:\n", "      the majority class in the *training* set, a string.\n", "    \"\"\"\n", "    ..."]}, {"cell_type": "code", "execution_count": null, "id": "6e89a6be", "metadata": {"deletable": false, "editable": false}, "outputs": [], "source": ["grader.check(\"majority_class\")"]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["Now we can see how well this majority class model works by trying it out on some examples. Use the `accuracy` function to determine the model's accuracy when applied to the task of labeling the _training_ data.\n", "<!--\n", "BEGIN QUESTION\n", "name: accuracy_maj_class_train\n", "-->"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#TODO -- Define `maj_class_on_train` to be the accuracy of the majority \n", "#        class model on the training data.\n", "accuracy_maj_class_train = ..."]}, {"cell_type": "code", "execution_count": null, "id": "38f99f4e", "metadata": {"deletable": false, "editable": false}, "outputs": [], "source": ["grader.check(\"accuracy_maj_class_train\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(f\"Accuracy of the majority class model on training data: \"\n", "      f\"{accuracy_maj_class_train:.3f}\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Was your guess from above right?"]}, {"cell_type": "markdown", "metadata": {}, "source": ["## Nearest neighbor classification\n", "\n", "Recall that nearest neighbor classification classifies a test example with the label of the nearest training example. To calculate nearest neighbors, we need a distance metric between the representations of the documents. Below we've provided two such metrics, familiar from the previous lab, for Euclidean distance and cosine distance.\n", "\n", "> Note: In order to allow full use of `torch` operations, these functions assume that the vectors are provided as tensors of type `float`. (That's why we tensorified the `counts` data as we loaded the dataset at the top  of this notebook.) When you call them, you'll want to make sure of this. They also return singleton tensors, not floats."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def euclidean_distance(v1, v2):\n", "    \"\"\"Returns the Euclidean distance between two vectors\"\"\"\n", "    return torch.linalg.norm(v1 - v2)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["def safe_acos(x):\n", "    \"\"\"Returns the arc cosine of `x`. Unlike `math.acos`, it \n", "       does not raise an exception for values of `x` out of range, \n", "       but rather clips `x` at -1..1, thereby avoiding math domain\n", "       errors in the case of numerical errors.\"\"\"\n", "    return math.acos(math.copysign(min(1.0, abs(x)), x))\n", "        \n", "def cosine_distance(v1, v2):\n", "    \"\"\"Returns the cosine distance between two vectors\"\"\"\n", "    dot_product = (v1 * v2).sum()\n", "    v1_norm = (v1 * v1).sum().sqrt()\n", "    v2_norm = (v2 * v2).sum().sqrt()\n", "    return safe_acos(dot_product / (v1_norm * v2_norm)) / math.pi"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Here's an example of the use of these distance metrics:"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["t1 = torch.tensor([1., 2.])\n", "t2 = torch.tensor([3., 4.])\n", "\n", "print(\"Testing on two different tensors\\n\"\n", "      f\"Euclidean: {euclidean_distance(t1, t2)}\\n\"\n", "      f\"Cosine   : {cosine_distance(t1, t2)}\\n\\n\"\n", "      \"Testing on two identical tensors\\n\"\n", "      f\"Euclidean: {euclidean_distance(t1, t1)}\\n\"\n", "      f\"Cosine   : {cosine_distance(t1, t1)}\")"]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["### Generating nearest neighbor models\n", "\n", "To specify a nearest neighbor model, we need both a training corpus (like `training`) and a distance metric (like `euclidean_distance` or `cosine_distance` defined just above). \n", "\n", "Define a function called `define_nearest_neighbor` that takes a training corpus and a distance metric and returns a model -- that is, a function that classifies a single test example. The model should return the class _label_ of that training example whose _counts vector_ is closest to that of the test example according to the metric.\n", "\n", "> Again, harkening to CS51, `define_nearest_neighbor` is a higher-order function since it _returns a function_. Yes, [higher-order functions are possible in Python](https://en.wikipedia.org/wiki/Higher-order_function#Python).\n", "<!--\n", "BEGIN QUESTION\n", "name: define_nearest_neighbor\n", "-->"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#TODO -- Define this function that generates nearest neighbor models.\n", "def define_nearest_neighbor(corpus, metric):\n", "    \"\"\"Generates a nearest neighbor model from a training corpus and a\n", "    distance metric.\n", "    Arguments:\n", "      `corpus`: a training corpus, such as `training`\n", "      `metric`: a metric function which takes two tensors as input and \n", "                returns their distance, such as `euclidean_distance`\n", "    Returns:\n", "      a model, which is a function that takes in a test example (such as \n", "      `testing[0]`) and returns the author of the nearest example in the \n", "      training set, where distances are measured on the counts vector \n", "      using `metric`.\n", "    \"\"\"\n", "    ..."]}, {"cell_type": "markdown", "metadata": {}, "source": ["We can use the `define_nearest_neighbor` function to define two new models for nearest neighbor classification, one using Euclidean distance and one using cosine distance."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["nearest_neighbor_euclidean_model = \\\n", "    define_nearest_neighbor(training, euclidean_distance)\n", "\n", "nearest_neighbor_cosine_model = \\\n", "    define_nearest_neighbor(training, cosine_distance)"]}, {"cell_type": "markdown", "metadata": {}, "source": ["### Testing the nearest neighbor models on the training data"]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["How accurate are these models when used to label the training data (as we did for the majority class model above)? Use the `accuracy` function above to calculate the accuracy of `nearest_neighbor_euclidean_model` in labeling the _training_ data (not the test data), and similarly for `nearest_neighbor_cosine_model`.\n", "<!--\n", "BEGIN QUESTION\n", "name: accuracy_train\n", "-->"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#TODO - Define the variable to be the calculated accuracy.\n", "accuracy_nn_euclidean_train = ...\n", "accuracy_nn_cosine_train = ..."]}, {"cell_type": "code", "execution_count": null, "id": "889ff5dc", "metadata": {"deletable": false, "editable": false}, "outputs": [], "source": ["grader.check(\"accuracy_train\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(f\"Accuracy of the nearest neighbor euclidean model tested on training data: \"\n", "      f\"{accuracy_nn_euclidean_train:.3f}\")\n", "print(f\"Accuracy of the nearest neighbor cosine model tested on training data: \"\n", "      f\"{accuracy_nn_cosine_train:.3f}\")"]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["<!-- BEGIN QUESTION -->\n", "\n", "**Question:** Does the performance of these classifiers on the training data seem to you to be representative of how good a classifier each is? Why or why not?\n", "<!--\n", "BEGIN QUESTION\n", "name: open_response_1\n", "manual: true\n", "-->"]}, {"cell_type": "markdown", "id": "4145219d", "metadata": {}, "source": ["_Type your answer here, replacing this text._"]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["<!-- END QUESTION -->\n", "\n", "#### Testing the nearest neighbor models on the testing data\n", "\n", "To get a better sense of how the nearest neighbor models perform, let's try them out on the testing data that we have. (Recall that the testing data in `testing` were the ambiguously-authored Federalist papers, where the `authors` field was `'Hamilton or Madison'`.)\n", "\n", "We start by looking in detail at the predictions generated by the two nearest neighbor models. Print out a table that lists, for each `testing` example, the paper number and the authors predicted under the nearest neighbor Euclidean model and the nearest neighbor cosine model. It might look something like\n", "```\n", "49 Madison  Madison \n", "50 Hamilton Madison \n", "51 Madison  Madison\n", "...\n", "```\n", "\n", "<!--\n", "BEGIN QUESTION\n", "name: print_table\n", "-->"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#TODO - Print out the requested table.\n", "..."]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["<!-- BEGIN QUESTION -->\n", "\n", "What do you notice about the two models?\n", "<!--\n", "BEGIN QUESTION\n", "name: open_response_2\n", "manual: true\n", "-->"]}, {"cell_type": "markdown", "id": "574b9485", "metadata": {}, "source": ["_Type your answer here, replacing this text._"]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["<!-- END QUESTION -->\n", "\n", "### Testing the nearest neighbor models on the training data\n", "\n", "Now use the `accuracy` function to calculate the accuracy of the two nearest neighbor models as you did above, but this time calculating accuracy on the *testing* corpus rather than the training corpus. (Expect to find a surprising result. Read ahead for an explanation if you're confused.)\n", "<!--\n", "BEGIN QUESTION\n", "name: accuracy_test\n", "-->"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#TODO -- Define the variables to be, respectively, the calculated accuracy of the nearest \n", "#        neighbor Euclidean model and cosine model on the testing data.\n", "accuracy_nn_euclidean_test = ...\n", "accuracy_nn_cosine_test = ..."]}, {"cell_type": "code", "execution_count": null, "id": "ad45a4b0", "metadata": {"deletable": false, "editable": false}, "outputs": [], "source": ["grader.check(\"accuracy_test\")"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(f\"Accuracy of the nearest neighbor euclidean model tested on testing data: \"\n", "      f\"{accuracy_nn_euclidean_test:.3f}\")\n", "print(f\"Accuracy of the nearest neighbor cosine model tested on testing data: \"\n", "      f\"{accuracy_nn_cosine_test:.3f}\")"]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["<!-- BEGIN QUESTION -->\n", "\n", "**Question:** Does the performance of these classifiers on the testing data seem to you to be representative of how good a classifier each is? Why or why not?\n", "<!--\n", "BEGIN QUESTION\n", "name: open_response_3\n", "manual: true\n", "-->"]}, {"cell_type": "markdown", "id": "0667d3ae", "metadata": {}, "source": ["_Type your answer here, replacing this text._"]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["<!-- END QUESTION -->\n", "\n", "### The importance of gold labels\n", "\n", "In order to evaluate the accuracy of the nearest neighbor model \u2013 and any model \u2013 we need to have the correct labels for the testing corpus, the so-called _gold_ labels. What shall we use for gold labels? Mosteller and Wallace's much more extensive analysis concluded that all of the papers of ambiguous origin were penned by Madison, so we'll use that. We should use a version of the `testing` corpus with the gold labels. \n", "\n", "Write some code to generate a version of the testing corpus with the gold labels.\n", "\n", "> Hint: In defining `testing_gold`, you'll want to be careful not to change `testing`. Otherwise, some unit tests that use `testing` may fail. The `copy.deepcopy` function may be useful.\n", "\n", "<!--\n", "BEGIN QUESTION\n", "name: get_gold\n", "-->"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["#TODO - Write code that defines `testing_gold`, which is the same\n", "# as `testing` except that it has the correct gold labels.\n", "# Note: be careful to not change `testing`.\n", "..."]}, {"cell_type": "code", "execution_count": null, "id": "2c28e8a7", "metadata": {"deletable": false, "editable": false}, "outputs": [], "source": ["grader.check(\"get_gold\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Now we can rerun the accuracy calculations."]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["accuracy_nn_euclidean_test_with_gold = accuracy(testing_gold, nearest_neighbor_euclidean_model)\n", "accuracy_nn_cosine_test_with_gold = accuracy(testing_gold, nearest_neighbor_cosine_model)"]}, {"cell_type": "code", "execution_count": null, "metadata": {}, "outputs": [], "source": ["print(f\"Accuracy of the nearest neighbor euclidean model tested on testing data: \"\n", "      f\"{accuracy_nn_euclidean_test_with_gold:.3f}\")\n", "print(f\"Accuracy of the nearest neighbor cosine model tested on testing data: \"\n", "      f\"{accuracy_nn_cosine_test_with_gold:.3f}\")"]}, {"cell_type": "markdown", "metadata": {}, "source": ["Do these results make more sense?"]}, {"cell_type": "markdown", "metadata": {"deletable": false, "editable": false}, "source": ["<!-- BEGIN QUESTION -->\n", "\n", "# Lab debrief \u2013 for consensus submission only\n", "\n", "**Question:** We're interested in any thoughts your group has about this lab so that we can improve this lab for later years, and to inform later labs for this year. Please list any issues that arose or comments you have to improve the lab. Useful things to comment on include the following: \n", "\n", "* Was the lab too long or too short?\n", "* Were the readings appropriate for the lab? \n", "* Was it clear (at least after you completed the lab) what the points of the exercises were? \n", "* Are there additions or changes you think would make the lab better?\n", "\n", "<!--\n", "BEGIN QUESTION\n", "name: open_response_debrief\n", "manual: true\n", "-->"]}, {"cell_type": "markdown", "id": "4473618a", "metadata": {}, "source": ["_Type your answer here, replacing this text._"]}, {"cell_type": "markdown", "metadata": {}, "source": ["<!-- END QUESTION -->\n", "\n", "\n", "\n", "# End of lab 1-2"]}, {"cell_type": "markdown", "id": "068e9554", "metadata": {"deletable": false, "editable": false}, "source": ["---\n", "\n", "To double-check your work, the cell below will rerun all of the autograder tests."]}, {"cell_type": "code", "execution_count": null, "id": "7cd0d16d", "metadata": {"deletable": false, "editable": false}, "outputs": [], "source": ["grader.check_all()"]}], "metadata": {"kernelspec": {"display_name": "Python 3", "language": "python", "name": "python3"}, "language_info": {"codemirror_mode": {"name": "ipython", "version": 3}, "file_extension": ".py", "mimetype": "text/x-python", "name": "python", "nbconvert_exporter": "python", "pygments_lexer": "ipython3", "version": "3.8.10"}, "title": "CS187 Lab 1-2: Text classification and evaluation methodology"}, "nbformat": 4, "nbformat_minor": 4}